from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import cv2
import mediapipe as mp
import numpy as np
import json
import time
import difflib
import csv
from moviepy import VideoFileClip
import speech_recognition as sr
import traceback
import pandas as pd
import random

app = Flask(__name__)
CORS(app)

UPLOAD_FOLDER = './uploaded_videos'
LANDMARKS_DIR = './guide_landmarks/guide_landmarks'
SYLLABLE_LANDMARKS_DIR = './guide_landmarks/ë°œìŒë…¹í™”_ìŒì ˆ'
EXCEL_PATH = './guide_landmarks/letters_for_letters.xlsx'
syllable_book = pd.read_excel(EXCEL_PATH, sheet_name=None)  # ëª¨ë“  ì‹œíŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë¶ˆëŸ¬ì˜´
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)

LIPS_IDX = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 324, 318, 402,
            317, 14, 87, 178, 88, 95, 185, 40, 39, 37, 0, 267, 269, 270, 409,
            415, 310, 311, 312, 13, 82, 81, 42, 183, 78]

current_signal = None


def load_landmarks(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return None

def compare_lip_shape(user_data, ref_data):
    user_array = np.array(user_data)  # (Nframes_user, 80)
    user_array = user_array.reshape(-1, 40, 2)
    avg_user = np.mean(user_array, axis=0)  # (40, 2)
    ref_points = np.array(ref_data)  # ì´ë¯¸ (40, 2)
    distances = np.linalg.norm(avg_user - ref_points, axis=1)
    return float(np.mean(distances))

def give_lip_feedback(distance):
    if distance < 0.13:
        return "ì •í™•í•´ìš”!"
    elif distance < 0.21:
        return "ë‚˜ì˜ì§€ ì•Šì•„ìš”!"
    else:
        return "ì…ëª¨ì–‘ì´ ë§ì´ ë‹¬ë¼ìš”."

def load_landmarks_from_csv(filepath):
    landmarks = []
    try:
        with open(filepath, newline='', encoding='utf-8') as csvfile:
            reader = csv.reader(csvfile)
            for row in reader:
                landmarks.append([float(val) for val in row])
        # landmarksëŠ” (Nframes, 80) ê°œ ì¢Œí‘œ(x,y*40) ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¼ ê²ƒ
        arr = np.array(landmarks)  # shape (Nframes, 80)
        if arr.shape[1] != 80:
            print(f"ê²½ê³ : CSV ë°ì´í„° ì—´ ê°œìˆ˜ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„: {arr.shape[1]}")
            return None
        arr = arr.reshape(-1, 40, 2)  # (Nframes, 40, 2)
        avg_landmarks = np.mean(arr, axis=0)  # (40, 2)
        return avg_landmarks.tolist()
    except Exception as e:
        print(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

def load_reference_phoneme(syllable):
    path = os.path.join(SYLLABLE_LANDMARKS_DIR, f"ì¢Œí‘œ_{syllable}_avg.csv")
    return load_landmarks_from_csv(path)

def analyze_syllables(user_text, current_data):
    syllables = list(user_text.replace(" ", ""))
    if not syllables:
        return "ì¸ì‹ëœ ìŒì ˆì´ ì—†ìŠµë‹ˆë‹¤."

    total_frames = len(current_data)
    total_syllables = len(syllables)
    if total_syllables == 1:
        syllable_frames = [current_data]
    else:
        frames_per_syllable = total_frames // total_syllables
        syllable_frames = [
            current_data[i * frames_per_syllable : (i + 1) * frames_per_syllable]
            for i in range(total_syllables)
        ]

    lip_msg = "\n[ì…ëª¨ì–‘ ë¶„ì„]\n"
    for idx, syllable in enumerate(syllables):
        syllable_lip_data = syllable_frames[idx] if idx < len(syllable_frames) else []
        ref_data = load_reference_phoneme(syllable)
        if ref_data:
            if syllable_lip_data:
                dist = compare_lip_shape(syllable_lip_data, ref_data)
                feedback = give_lip_feedback(dist)
                lip_msg += f"- '{syllable}' â†’ ê±°ë¦¬: {dist:.4f}, í”¼ë“œë°±: {feedback}\n"
            else:
                lip_msg += f"- '{syllable}' â†’ ì‚¬ìš©ì ë°ì´í„° ë¶€ì¡±\n"
        else:
            lip_msg += f"- '{syllable}' â†’ ê¸°ì¤€ ë°ì´í„° ì—†ìŒ\n"
    return lip_msg

def get_word_from_initials(initials):
    if initials not in syllable_book:
        return None
    words = syllable_book[initials].dropna().values.flatten().tolist()
    return random.choice(words) if words else None

def extract_text_from_video(video_path):
    try:
        with sr.AudioFile(audio_path) as source:
            audio = recognizer.record(source)
            real_sound = recognizer.recognize_google(audio, language="ko-KR")
    except Exception as e:
        messagebox.showerror("ìŒì„± ì¸ì‹ ì˜¤ë¥˜", f"ì˜¤ë¥˜: {e}")
        return

    status_label.config(text=f"ì¸ì‹ëœ ë¬¸ì¥: {user_text}")


def analyze_word_similarity(user_text, target_text):
    user_words = user_text.split()
    target_words = target_text.split()
    used_indices = set()
    word_feedback = []

    for idx, t_word in enumerate(target_words):
        best_match = None
        best_score = 0
        best_idx = -1
        for i, u_word in enumerate(user_words):
            if i in used_indices:
                continue
            score = difflib.SequenceMatcher(None, t_word, u_word).ratio()
            if score > best_score:
                best_score = score
                best_match = u_word
                best_idx = i

        if best_match is not None:
            used_indices.add(best_idx)
            pos = "ìœ„ì¹˜ ì¼ì¹˜" if best_idx == idx else "ìœ„ì¹˜ ë°”ë€œ"

            # ğŸ” ìƒì„¸ í”¼ë“œë°± ìƒì„±
            hint = ""
            if t_word != best_match:
                if t_word.startswith(best_match):
                    hint = " â†’ ë ì†Œë¦¬ê°€ ë¹ ì¡Œì„ ìˆ˜ ìˆì–´ìš”."
                elif best_match.startswith(t_word):
                    hint = " â†’ ëì— ë¶ˆí•„ìš”í•œ ì†Œë¦¬ê°€ ë¶™ì—ˆì„ ìˆ˜ ìˆì–´ìš”."
                elif sorted(t_word) == sorted(best_match):
                    hint = " â†’ ì² ìëŠ” ë¹„ìŠ·í•˜ì§€ë§Œ ìˆœì„œê°€ ë°”ë€ ê²ƒ ê°™ì•„ìš”."
                else:
                    hint = " â†’ ë°œìŒì´ ë¹„ìŠ·í•˜ì§€ë§Œ ë‹¤ë¥¸ ë‹¨ì–´ì¼ ìˆ˜ ìˆì–´ìš”."

            word_feedback.append(
                f"- '{t_word}' â†” '{best_match}' : {best_score*100:.1f}% ({pos}){hint}"
            )
        else:
            word_feedback.append(f"- '{t_word}' â†” (ëˆ„ë½ë¨) : 0.0%")

    extra_words = [w for i, w in enumerate(user_words) if i not in used_indices]
    if extra_words:
        word_feedback.append(f"\nì¶”ê°€ë¡œ ë§í•œ ë‹¨ì–´ë“¤: {' '.join(extra_words)}")

    return word_feedback

def analyze_lip_movement(video_path, expected_mouth):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return {'error': 'ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}

    lip_data = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb)
        if results.multi_face_landmarks:
            face = results.multi_face_landmarks[0]
            lip_coords = [coord for idx in LIPS_IDX for coord in (face.landmark[idx].x, face.landmark[idx].y)]
            lip_data.append(lip_coords)
    cap.release()

    standard_path = os.path.join(LANDMARKS_DIR, f"guide_landmarks_{expected_mouth}.json")
    print(f"í‘œì¤€ ê²½ë¡œ: {standard_path}")
    if not os.path.exists(standard_path):
        return {'error': f"í‘œì¤€ íŒŒì¼ '{standard_path}' ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}

    standard_data = load_landmarks(standard_path)
    if not standard_data:
        return {'error': f"íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨: '{standard_path}'"}
    if 'lips' not in standard_data:
        return {'error': f"'lips' í‚¤ê°€ '{standard_path}'ì— ì—†ìŠµë‹ˆë‹¤."}

    standard_lips = standard_data['lips']
    if len(standard_lips) != len(LIPS_IDX):
        return {'error': f"í‘œì¤€ ì…ìˆ  ëœë“œë§ˆí¬ ê°œìˆ˜ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤."}

    total_similarity = 0
    max_distance = 0.1
    for frame_lip_coords in lip_data:
        user_lip_points = np.array(frame_lip_coords).reshape(-1, 2)
        standard_lip_points = np.array([standard_lips[i] for i in range(len(LIPS_IDX))])
        distances = np.linalg.norm(user_lip_points - standard_lip_points, axis=1)
        avg_distance = np.sqrt(np.mean(distances**2))
        if avg_distance > max_distance:
            similarity = 0
        else:
            similarity = 1 - (avg_distance / max_distance) ** 2
        total_similarity += similarity * 100

    average_similarity = total_similarity / len(lip_data)
    return {
    'accuracy': f"{average_similarity:.2f}",
    'lip_data': lip_data  # syllable ë¶„ì„ìš©
}

@app.route('/signal', methods=['POST'])
def receive_signal():
    global current_signal
    data = request.get_json()
    signal = data.get('signal')
    if signal in [0, 1,2, 3, 4]:
        current_signal = signal
        return jsonify({'message': f'Signal {signal} received'}), 200
    else:
        return jsonify({'error': 'Invalid signal'}), 400

@app.route('/upload_and_analyze', methods=['POST'])
def upload_and_analyze():
    global current_signal
    if current_signal != 2:
        return jsonify({'error': 'Signal 2ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 403
    current_signal = None
    file = request.files.get('file')
    expected_mouth = request.form.get('mouth')
    if not file or not expected_mouth:
        return jsonify({'error': 'íŒŒì¼ ë˜ëŠ” mouth ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'}), 400

    filename = f"uploaded_{expected_mouth}_{int(time.time())}.mp4"
    save_path = os.path.join(UPLOAD_FOLDER, filename)
    file.save(save_path)
    result = analyze_lip_movement(save_path, expected_mouth)
    print("ìµœì¢… ë¶„ì„ ë©”ì‹œì§€:\n", result)
    os.remove(save_path)
    return jsonify(result)
    
@app.route('/get_word', methods=['POST'])
def get_word():
    initials = request.json.get('initials', '')
    if not initials:
        return jsonify({'error': 'ì´ˆì„±ì´ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
    word = get_word_from_initials(initials)
    if not word:
        return jsonify({'error': f"ì´ˆì„± '{initials}'ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ ì—†ìŒ"}), 404
    return jsonify({'word': word})

@app.route('/upload_and_analyze_syllable', methods=['POST'])
def upload_and_analyze_syllable():
    global current_signal
    if current_signal != 3:
        return jsonify({'error': 'Signal 3ì´ í•„ìš”í•©ë‹ˆë‹¤.'}), 403
    current_signal = None

    file = request.files.get('file')
    user_text = request.form.get('user_text', '')
    if not file or not user_text:
        return jsonify({'error': 'íŒŒì¼ ë˜ëŠ” ë‹¨ì–´ ëˆ„ë½'}), 400

    os.makedirs(UPLOAD_FOLDER, exist_ok=True)
    filename = f"syllable_{int(time.time())}.mp4"
    save_path = os.path.join(UPLOAD_FOLDER, filename)
    file.save(save_path)

    # ì˜ìƒ ì²˜ë¦¬
    cap = cv2.VideoCapture(save_path)
    current_data = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb)  # ë¯¸ë””ì–´íŒŒì´í”„ ê°ì²´ê°€ ì´ˆê¸°í™”ë˜ì–´ ìˆì–´ì•¼ í•¨
        if results.multi_face_landmarks:
            face = results.multi_face_landmarks[0]
            lip_coords = [coord for idx in LIPS_IDX for coord in (face.landmark[idx].x, face.landmark[idx].y)]
            current_data.append(lip_coords)
    cap.release()
    os.remove(save_path)

    syllable_result = analyze_syllables(user_text, current_data)
    return jsonify({
        'syllable_result': syllable_result,
        'selected_word': user_text  # í”„ë¡ íŠ¸ì— ë‹¨ì–´ í‘œì‹œë¥¼ ìœ„í•œ ë°˜í™˜
    })

@app.route('/upload_and_analyze_sentence', methods=['POST'])
def upload_and_analyze_sentence():
    try:
        global current_signal
        if current_signal != 4:
            return jsonify({'error': 'Signal 4ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 403
        current_signal = None

        file = request.files.get('file')
        user_text = request.form.get('user_text', '')  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì •ë‹µ ë¬¸ì¥

        if not file or not user_text:
            return jsonify({'error': 'íŒŒì¼ ë˜ëŠ” user_text ëˆ„ë½'}), 400

        save_path = 'temp_video.mp4'
        wav_path = 'temp_audio.wav'
        file.save(save_path)

        # ì˜¤ë””ì˜¤ ì¶”ì¶œ
        clip = VideoFileClip(save_path)
        clip.audio.write_audiofile(wav_path)
        clip.reader.close()
        recognizer = sr.Recognizer()
        with sr.AudioFile(wav_path) as source:
            audio = recognizer.record(source)
            realsound = recognizer.recognize_google(audio, language='ko-KR')
        os.remove(wav_path)

        ########## ì „ì²´ ë¬¸ì¥ ë°œìŒ ì •í™•ë„ ##########
        accuracy = difflib.SequenceMatcher(None, realsound, user_text).ratio() * 100
        message = f"ì „ì²´ ë¬¸ì¥ ë°œìŒ ì •í™•ë„: {accuracy:.2f}%\n\n"

        ########## ë‹¨ì–´ë³„ í”¼ë“œë°± ##########
        real_words = realsound.split()
        target_words = user_text.split()
        used_indices = set()
        word_feedback = []

        for idx, t_word in enumerate(target_words):
            best_match = None
            best_score = 0
            best_idx = -1
            for i, r_word in enumerate(real_words):
                if i in used_indices:
                    continue
                score = difflib.SequenceMatcher(None, t_word, r_word).ratio()
                if score > best_score:
                    best_score = score
                    best_match = r_word
                    best_idx = i
            if best_match is not None:
                used_indices.add(best_idx)
                pos = "ìœ„ì¹˜ ì¼ì¹˜" if best_idx == idx else "ìœ„ì¹˜ ë°”ë€œ"
                word_feedback.append(f"- '{t_word}' â†” '{best_match}' : {best_score*100:.1f}% ({pos})")
            else:
                word_feedback.append(f"- '{t_word}' â†” (ëˆ„ë½ë¨) : 0.0%")

        extra_words = [w for i, w in enumerate(real_words) if i not in used_indices]
        if extra_words:
            word_feedback.append(f"\nì¶”ê°€ë¡œ ë§í•œ ë‹¨ì–´ë“¤: {' '.join(extra_words)}")

        message += "[ë‹¨ì–´ë³„ ë¶„ì„]\n" + "\n".join(word_feedback)

        ########## ì…ëª¨ì–‘ ë°ì´í„° ìˆ˜ì§‘ ##########
        cap = cv2.VideoCapture(save_path)
        current_data = []
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(rgb)
            if results.multi_face_landmarks:
                face = results.multi_face_landmarks[0]
                lip_coords = [coord for idx in LIPS_IDX for coord in (face.landmark[idx].x, face.landmark[idx].y)]
                current_data.append(lip_coords)
        cap.release()
        os.remove(save_path)

        ########## ìŒì ˆë³„ ì…ëª¨ì–‘ ë¶„ì„ ##########
        lip_msg = "\n\n[ì…ëª¨ì–‘ ë¶„ì„]\n"
        syllables = list(user_text.replace(" ", ""))  # ê¸°ì¤€ ë¬¸ì¥ì˜ ìŒì ˆë¡œ ë¶„ì„

        if not syllables:
            lip_msg += "ì¸ì‹ëœ ìŒì ˆì´ ì—†ìŠµë‹ˆë‹¤.\n"
        else:
            total_frames = len(current_data)
            total_syllables = len(syllables)

            if total_syllables == 1:
                syllable_frames = [current_data]
            else:
                frames_per_syllable = total_frames // total_syllables
                syllable_frames = [
                    current_data[i * frames_per_syllable : (i + 1) * frames_per_syllable]
                    for i in range(total_syllables)
                ]

            for idx, syllable in enumerate(syllables):
                syllable_lip_data = syllable_frames[idx] if idx < len(syllable_frames) else []
                ref_data = load_reference_phoneme(syllable)
                if ref_data:
                    if syllable_lip_data:
                        dist = compare_lip_shape(syllable_lip_data, ref_data)
                        feedback = give_lip_feedback(dist)
                        lip_msg += f"- '{syllable}' â†’ ê±°ë¦¬: {dist:.4f}, í”¼ë“œë°±: {feedback}\n"
                    else:
                        lip_msg += f"- '{syllable}' â†’ ì‚¬ìš©ì ë°ì´í„° ë¶€ì¡±\n"
                else:
                    lip_msg += f"- '{syllable}' â†’ ê¸°ì¤€ ë°ì´í„° ì—†ìŒ\n"

        message += lip_msg if lip_msg else "\nê¸°ì¤€ ì¢Œí‘œê°€ ì—†ëŠ” ìŒì ˆì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."

        return jsonify({'message': message, 'accuracy': f"{accuracy:.2f}"})
    
    except Exception as e:
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    if not os.path.exists(LANDMARKS_DIR):
        print(f"ê²½ê³ : ê¸°ì¤€ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ â†’ {LANDMARKS_DIR}")
    else:
        print(f"ğŸ“ ê¸°ì¤€ ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸ë¨: {LANDMARKS_DIR}")
    port = int(os.environ.get("PORT", 5000))  # Renderê°€ PORT í™˜ê²½ë³€ìˆ˜ë¥¼ ì œê³µ
    app.run(host="0.0.0.0", port=port)
